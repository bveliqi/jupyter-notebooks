{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets [Network Def., Loss Fn., Backprop., Optimizers]\n",
    "\n",
    "Neural networks can be constructed using the `torch.nn` package.\n",
    "\n",
    "Now that you had a glimpse of autograd, nn depends on autograd to define models and differentiate them. \n",
    "An `nn.Module` contains layers, and a method `forward(input)` that returns the `output`.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    "<img src=\"img/mnist.png\" />\n",
    "\n",
    "[MNIST Example]\n",
    "\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
    "\n",
    "A **typical training procedure** for a neural network is as follows:\n",
    "\n",
    "1. Define the neural network that has some learnable parameters (or weights)\n",
    "2. Iterate over a dataset of inputs\n",
    "3. Process input through the network\n",
    "4. Compute the loss (how far is the output from being correct)\n",
    "5. Propagate gradients back into the network’s parameters\n",
    "6. Update the weights of the network, typically using a simple update rule: \n",
    "   `weight = weight - learning_rate * gradient`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b (?)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) # i.e. 10 classes/digits\n",
    "        \n",
    "    def forward(self, x: Variable) -> Variable:\n",
    "        # the input & output of the forward-fn is an autograd.Variable\n",
    "        # max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # if the size is a square you can also only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # no act. func. for the last one to always generate an output\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x: Variable) -> int:\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# the learnable parameters of a model are returned by net.parameters()\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0577  0.1013 -0.1042 -0.0948 -0.0703  0.0943  0.0724 -0.1412 -0.0528 -0.0032\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(out)\n",
    "\n",
    "# zero the gradient buffers of all params and backprops with rndm gradients:\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Note:\n",
    "\n",
    "`torch.nn` only supports mini-batches. \n",
    "The entire `torch.nn` package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "For example, `nn.Conv2d` will take in a 4D Tensor of `nSamples x nChannels x Height x Width`.\n",
    "\n",
    "\n",
    "### Recap\n",
    "\n",
    "- `torch.Tensor`: a multi-dim. array\n",
    "- `autograd.Variable`: wraps a Tensor, records the history of ops applied to it. Same API as a Tensor with some additions like `.backward()`. Holds also gradient w.r.t. the tensor.\n",
    "- `nn.Module`: convient way of encapsulating params, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "- `nn.Parameter`: a kind of `Variable`, that is automatically registered as a parameter when assigned as an attribute to a `Module`.\n",
    "- `autograd.Function`: impl. forward and backward definitions of an autograd operation. Every `Variable` op creates at least a single `Function` node, that connects to functions that created a `Variable`and encodes its history.\n",
    "\n",
    "\n",
    "** ⬆️ Until now:**\n",
    "- defining a neural net\n",
    "- processing inputs and calling backward\n",
    "\n",
    "** ⬇️ Now: **\n",
    "- computing the loss\n",
    "- updating the weights of the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "- takes the (output, target)-pair of inputs\n",
    "- computes a value to estimate how far away the calculated output it from the actual target \n",
    "- there are several different loss functions under the nn package\n",
    "- `nn.MSELoss` is an example loss function which compute tne mean-squared error between input + target\n",
    "- See: http://pytorch.org/docs/stable/nn.html#loss-functions for more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.8000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = Variable(torch.arange(1, 11)) # dummy target\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our computation graph would now look like this:\n",
    "\n",
    "```\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "\n",
    "- to backpropagate the error all we have to do is: `loss.backward()`\n",
    "- need to **clean the existing gradients first**, otherwise they will be accumulated to existing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      "-0.0084\n",
      "-0.0171\n",
      "-0.0383\n",
      " 0.0250\n",
      " 0.1337\n",
      " 0.0375\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read later**:\n",
    "neural net package contains various modules and loss functions that form the building blocks of deep neural nets, full doc of the package is here: http://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "\n",
    "Now - Last thing left, for a complete model:\n",
    "\n",
    "### Update the weights:\n",
    "\n",
    "- **optimizers == update rules for the model**\n",
    "- simplest update rule used in practice is `Stochastic Gradient Descent` (**SGD**): `weight = weight - learning_rate * gradient`\n",
    "- other examples: Nesterov-SGD, Adam, RMSProp, ...\n",
    "- see `torch.optim` package to see them all: http://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create an optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# training loop:\n",
    "optimizer.zero_grad() # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step() # do the actual update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
